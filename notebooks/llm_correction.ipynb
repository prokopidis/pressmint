{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bee832e",
   "metadata": {},
   "source": [
    "# LLM OCR Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3cb39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646abeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from tqdm.notebook import tqdm # Import tqdm for notebooks\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    TesseractCliOcrOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    )\n",
    "\n",
    "dotenv.load_dotenv(Path.home() / '.env')\n",
    "try:\n",
    "        # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    if (not \"NSK_VLLM_KEY\" in os.environ) or os.environ[\"NSK_VLLM_KEY\"]==None or os.environ[\"NSK_VLLM_KEY\"]==\"\" or os.environ[\"NSK_VLLM_KEY\"]==\"EMPTY\":\n",
    "        client = openai.OpenAI(\n",
    "            base_url=os.environ[\"NSK_VLLM_BASE_URL\"]\n",
    "        )\n",
    "        DEFAULT_MODEL = \"google/gemma-3-27b-it\"\n",
    "\n",
    "    else:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=os.environ[\"NSK_VLLM_KEY\"],\n",
    "            base_url=os.environ[\"NSK_VLLM_BASE_URL\"]\n",
    "        )\n",
    "except:\n",
    "    api_key=os.environ[\"LLM_PROXY_ILSP_EVAL_API_KEY\"]\n",
    "    base_url=os.environ[\"LLM_PROXY_ILSP_BASE_URL\"]\n",
    "    client = openai.OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=base_url,\n",
    "    )\n",
    "\n",
    "if not DEFAULT_MODEL: \n",
    "    models_to_test = [\"gemma3-27b-it\", 'krikri-dpo-1560', \"llama-3.1-8b\"]\n",
    "    DEFAULT_MODEL = models_to_test[0]\n",
    "\n",
    "print(DEFAULT_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4d3ec",
   "metadata": {},
   "source": [
    "## Defaults, template and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eee264",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(Path.home() / '.env')\n",
    "\n",
    "try:\n",
    "        # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    if (not \"NSK_VLLM_KEY\" in os.environ) or os.environ[\"NSK_VLLM_KEY\"]==None or os.environ[\"NSK_VLLM_KEY\"]==\"\" or os.environ[\"NSK_VLLM_KEY\"]==\"EMPTY\":\n",
    "        client = openai.OpenAI(\n",
    "            base_url=os.environ[\"NSK_VLLM_BASE_URL\"]\n",
    "        )\n",
    "        DEFAULT_MODEL = \"google/gemma-3-27b-it\"\n",
    "    else:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=os.environ[\"NSK_VLLM_KEY\"],\n",
    "            base_url=os.environ[\"NSK_VLLM_BASE_URL\"]\n",
    "        )\n",
    "except:\n",
    "    api_key=os.environ[\"LLM_PROXY_ILSP_EVAL_API_KEY\"]\n",
    "    base_url=os.environ[\"LLM_PROXY_ILSP_BASE_URL\"]\n",
    "    client = openai.OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=base_url,\n",
    "    )\n",
    "\n",
    "models_to_test = [\"gemma3-27b-it\", 'krikri-dpo-1560', \"llama-3.1-8b\"]\n",
    "if not DEFAULT_MODEL:\n",
    "    DEFAULT_MODEL = models_to_test[1]\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an AI assistant specialized in correcting polytonic Greek text created via OCR. Always provide your output in the specified JSON format.\"\n",
    "DEFAULT_USER_PROMPT = \"\"\"Your input text is polytonic Greek text created via OCR. You have to read the input text and generate a corrected version without the errors created by the OCR process.  \n",
    "\n",
    "You should return polytonic Greek text. You should NOT convert the input text to monotonic Greek. For example, you should not change  \"ΌΡΟΙ\" to \"ΟΡΟΙ\" if  \"ΌΡΟΙ\" is correct polytonic Greek.\n",
    "\n",
    "You should not change markdown headers or tables in the input text.  For example, you should not change  \"##\" to \"#\" in a Markdown header.\n",
    "\n",
    "You also have to provide a corrections list in Markdown explaining your corrections. Apart from this list, in your answer do NOT explain what you plan to do; simply return the output in the specified JSON format.\"\"\"\n",
    "\n",
    "class PolytonicSpeechCorrection(BaseModel):\n",
    "    corrections_list: str = Field(description=\"Markdown list with the corrections applied in the polytonic input text.\")\n",
    "    corrected_text: str = Field(description=\"The corrected polytonic text.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=PolytonicSpeechCorrection)\n",
    "\n",
    "template = DEFAULT_USER_PROMPT + \"\\n\" + \"\"\"\n",
    "\n",
    "---\n",
    "Polytonic Greek text created via OCR:\n",
    "{ocr_text}\n",
    "---\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    client,\n",
    "    model=DEFAULT_MODEL,\n",
    "    system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "    user_prompt=DEFAULT_USER_PROMPT,\n",
    "    temperature=0.1, \n",
    "    max_tokens=5000,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates text using the litellm library (or any OpenAI-compatible client).\n",
    "\n",
    "    Args:\n",
    "        client (openai.OpenAI): The initialized OpenAI client connected to LiteLLM proxy.\n",
    "        model (str): The language model to use.\n",
    "        system_prompt (str): The system prompt for the model.\n",
    "        user_prompt (str): The user's input prompt.\n",
    "        temperature (float): Controls the randomness of the output.\n",
    "        max_tokens (int): Limits the length of the generated response.\n",
    "        top_p (float): Controls nucleus sampling.\n",
    "        frequency_penalty (float): Penalizes repeated tokens.\n",
    "        presence_penalty (float): Penalizes new tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text, or None if there was an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def split_markdown_file_into_chunks(file_path: str, chunk_size: int = 2500) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits a Markdown file into chunks of approximately 'chunk_size' words.\n",
    "    The splitting occurs at the newline closest to the word limit.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the Markdown file.\n",
    "        chunk_size (int): The target word limit for each chunk. Defaults to 2500.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of strings, where each string is a chunk of the Markdown file.\n",
    "                   Returns an empty list if the file cannot be read or is empty.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        return []\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Error: '{file_path}' is not a file.\")\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_lines = []\n",
    "    current_chunk_word_count = 0\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Calculate word count for the current line\n",
    "                # Splitting by default splits by whitespace and handles multiple spaces\n",
    "                line_word_count = len(line.split())\n",
    "\n",
    "                # Check if adding this line would push the current chunk significantly over the limit\n",
    "                # and if the current chunk already has content.\n",
    "                # This ensures we don't create an empty chunk if the first line is huge.\n",
    "                if current_chunk_word_count + line_word_count > chunk_size and current_chunk_lines:\n",
    "                    # If so, finalize the current chunk before adding the new line\n",
    "                    chunks.append(\"\".join(current_chunk_lines))\n",
    "                    # Reset for the new chunk, starting with the current line\n",
    "                    current_chunk_lines = [line]\n",
    "                    current_chunk_word_count = line_word_count\n",
    "                else:\n",
    "                    # Otherwise, add the line to the current chunk\n",
    "                    current_chunk_lines.append(line)\n",
    "                    current_chunk_word_count += line_word_count\n",
    "\n",
    "            # After the loop, add any remaining content as the last chunk\n",
    "            if current_chunk_lines:\n",
    "                chunks.append(\"\".join(current_chunk_lines))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return []\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_markdown_file(md_path, n=30, model_name=DEFAULT_MODEL, temperature=0.1):\n",
    "    correction_results = []\n",
    "    with open(md_path, encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = split_markdown_file_into_chunks(str(md_path), chunk_size = 250)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_stripped = chunk.strip()\n",
    "        logger.info(f\"Processing chunk #{idx+1} of {len(chunks)} of {md_path}\")\n",
    "        try:\n",
    "            formatted_user_prompt = template.format(\n",
    "                ocr_text = chunk_stripped,\n",
    "                format_instructions=parser.get_format_instructions()\n",
    "            )\n",
    "            raw_llm_output = generate_text(client=client,  model=model_name, system_prompt=DEFAULT_SYSTEM_PROMPT, user_prompt=formatted_user_prompt, temperature=temperature )\n",
    "            # Parse the raw LLM output into our Pydantic model\n",
    "            parsed_output = parser.parse(raw_llm_output)\n",
    "            correction_result = {\n",
    "                \"corrected_text\": parsed_output.corrected_text,\n",
    "                \"corrections_list\": parsed_output.corrections_list,\n",
    "                \"status\": \"success\"\n",
    "            } \n",
    "            logger.info(f\"Success in processing chunk #{idx+1}\")\n",
    "        except Exception as e:\n",
    "            correction_result = {\n",
    "                \"corrected_text\" : None,\n",
    "                \"corrections_list\" : [],\n",
    "                \"status\" :  f\"failure:  General error during analysis: {e}\"\n",
    "            } \n",
    "            logger.info(f\"Failure in processing chunk #{idx+1}\")\n",
    "        correction_result.update({\n",
    "            \"filename\": str(md_path),\n",
    "            \"chunk_index\": idx,\n",
    "            \"original_text\": chunk_stripped\n",
    "        })\n",
    "        # logger.info(correction_result)\n",
    "        correction_results.append(correction_result)\n",
    "    return correction_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b1888",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"..\") / \"data/Newspapers/\"\n",
    "subdirs = [p for p in data_folder.iterdir() if p.is_dir()]\n",
    "\n",
    "markdown_paths = []\n",
    "filenames_to_check = [\"124_6_-1.docling.md\",  \"123_449_-1.docling.md\",  \"65_1_-1.docling.md\", \"108_2378_-1.docling.md\", \"5009_51.docling.md\", \n",
    "                      \"arc-2005-8632.docling.md\", \"Athinai_E_1907-1913_ar 17.docling.md\"\n",
    "                      ]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    md_paths = list(subdir.glob(\"*.docling.md\"))\n",
    "    if filenames_to_check:\n",
    "        for md_path in md_paths:\n",
    "            if md_path.name in filenames_to_check:\n",
    "                markdown_paths.append(md_path)\n",
    "    else:\n",
    "        for md_path in md_paths:\n",
    "            markdown_paths.append(md_path)\n",
    "        \n",
    "\n",
    "for md_path in markdown_paths:  \n",
    "    corrected_json_path = md_path.parent / (md_path.stem + f\".corrected.json\")\n",
    "    if corrected_json_path.exists():\n",
    "        logger.info(f\"Skipping processing already processed {md_path}\")\n",
    "        continue\n",
    "    else:\n",
    "        logger.info(f\"Processing {md_path} with {client.base_url}/{DEFAULT_MODEL}\")\n",
    "        file_results = process_markdown_file(md_path)\n",
    "        try:\n",
    "            with open(corrected_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(file_results, f, indent=4, ensure_ascii=False)\n",
    "                print(f\"Exported results to {corrected_json_path} \")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving file: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(all_results)\n",
    "# df.to_excel(Path.home() / \"llm_correction_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pressmint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
